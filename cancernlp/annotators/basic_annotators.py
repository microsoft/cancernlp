import re
from typing import Optional, Type, Union

from more_itertools import chunked

from ..annotation import (AnnotatedText, AnnotatedTextInterface, Annotation,
                          Sentence, Token)


class SimpleToken(Token):
    """Token generated by SimpleTokenizer"""


class SimpleSentence(Sentence):
    """Sentence generated by SimpleTokenizer"""


class SimpleTokenizer:
    def __init__(self, add_tokens: bool = True, add_sentences: bool = True):
        self.add_tokens = add_tokens
        self.add_sentences = add_sentences

    def __call__(self, ann_text: AnnotatedText) -> None:
        text = ann_text.text
        tokens = []
        if self.add_tokens:
            tokens = [
                SimpleToken(begin=match.start(), end=match.end(), text=match.group())
                for match in re.finditer(r"\S+", text)
            ]
        sentences = []
        if self.add_sentences:
            # skip any initial whitespace before first sentence
            cur_begin = re.search(r"^\s*", text).end()
            # look for end-of-sentence punctuation
            for sentence_break in re.finditer(r"([.?!]\"?)\s+", text):
                end = sentence_break.start() + len(sentence_break.group(1))
                sentences.append(SimpleSentence(begin=cur_begin, end=end))
                cur_begin = sentence_break.end()
            # add last sentence
            last_sent = text[cur_begin:].strip()
            if last_sent:
                begin = text.find(last_sent, cur_begin)
                end = begin + len(last_sent)
                sentences.append(SimpleSentence(begin=begin, end=end))

        ann_text.add_annotations(tokens + sentences)


class MaxTokensAnnotator:
    """Annotator that enforces a maximum number of tokens per sentence (or sentences
    per doc) by removing excess annotations"""

    def __init__(
        self,
        max_tokens: int,
        token_type: Union[str, Type[Annotation]],
        sentence_type: Optional[Union[str, Type[Annotation]]] = None,
    ):
        self.max_tokens = max_tokens
        self.token_type = token_type
        self.sentence_type = sentence_type

    def __call__(self, ann_text: AnnotatedTextInterface) -> None:
        if self.sentence_type:
            sentences = ann_text.covering(self.sentence_type)
        else:
            sentences = [ann_text]

        tokens_to_remove = []
        for sentence in sentences:
            tokens = sentence.covering(self.token_type)
            tokens_to_remove.extend(
                [_t for _idx, _t in enumerate(tokens) if _idx >= self.max_tokens]
            )
        ann_text.remove_annotations(tokens_to_remove)


class FixedLengthSentence(Sentence):
    """Annotation type for sentences created to group a fixed number of tokens in
    each sentence."""


class FixedLengthSentencizer:
    """Adds sentence annotations that group tokens into fixed-length groups."""

    def __init__(self, token_type: Union[str, Type[Annotation]], sentence_length: int):
        self.token_type = token_type
        self.sentence_length = sentence_length

    def __call__(self, ann_text: AnnotatedTextInterface) -> None:
        tokens = ann_text.covering(self.token_type)
        grouped_tokens = chunked(tokens, self.sentence_length)
        sentences = [
            FixedLengthSentence(begin=token_group[0].begin, end=token_group[-1].end)
            for token_group in grouped_tokens
        ]
        ann_text.add_annotations(sentences)
